import os
import yaml
import glob

OUTDIR_RAW = os.path.join(os.getcwd(), "get_cami2_data", "raw_data")
OUTDIR_PROCESSED = os.path.join(os.getcwd(), "get_cami2_data", "processed_data")
OUTDIR_MODEL_RESULTS = os.path.join(os.getcwd(), "get_cami2_data", "model_results")
OUTDIR_CLUSTERS = os.path.join(os.getcwd(), "get_cami2_data", 'clusters')
SRC_DIR = os.path.join(os.getcwd(), "get_cami2_data", "src")
BASE = os.path.join(os.getcwd(), "get_cami2_data")

with open(os.path.join(BASE, "config.yml"), "r") as f:
    datasets_config = yaml.safe_load(f)

ALL_DATASETS = [d["name"] for d in datasets_config["datasets"]]
DATASET = config.get("DATASET", "all")

SAMPLES = {d["name"]: d.get("samples", []) for d in datasets_config["datasets"]}
sample_str = ' '.join(SAMPLES.get(DATASET, []))

ALL_MODELS  = [d["name"] for d in datasets_config["models"]]
config_model = config.get("MODEL", "all")
MODELS = config_model if config_model != 'all' else ALL_MODELS

#input:
#sample reads and contigs
# expand(os.path.join(OUTDIR_TMP, DATASET, "{sample}_contigs.fasta"), sample=[sample for sample in SAMPLES.get(DATASET, [])]),
# expand(os.path.join(OUTDIR_TMP, DATASET, "{sample}_reads.fq.gz"),  sample=[sample for sample in SAMPLES.get(DATASET, [])]),

#catalogue
# os.path.join(OUTDIR, DATASET, "catalogue.fna.gz")

#bams
#expand(os.path.join(OUTDIR, DATASET, "{sample}_sorted.bam"), sample=[sample for sample in SAMPLES.get(DATASET, [])])

#create fasta bins
#os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "all_clusters"),
#expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "fasta_bins", "{model}"), model=MODELS))

rule all:
    input:
        #checkm2
        os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "checkm2_database_downloaded.txt"),
        expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "checkm2", "{model}_results"), model=MODELS)


        


# rule download_name:
#     output:
#         contigs=expand(os.path.join(OUTDIR_RAW, DATASET, "{sample}_contigs.fasta"), sample=SAMPLES.get(DATASET, [])),
#         reads=expand(os.path.join(OUTDIR_RAW, DATASET, "{sample}_reads.fq.gz"), sample=SAMPLES.get(DATASET, []))
#     conda:
#         "envs/cami2_processing.yml"
#     shell:
#         """
#         python {SRC_DIR}/get_cami2_data.py {DATASET} {sample_str}
#         """



# rule concatenate:
#     input:
#         expand(os.path.join(OUTDIR_RAW, DATASET, "{sample}_contigs.fasta"), sample=SAMPLES.get(DATASET, []))
#     output:
#         os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz")
#     conda:
#         "envs/vamb.yml"
#     shell:
#         """
#         python {SRC_DIR}/concatenate.py {output} {input}
#         """


# rule alignment:
#     input:
#         read=os.path.join(OUTDIR_RAW, DATASET, "{sample}_reads.fq.gz"),
#         catalogue=os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz")
#     output:
#         os.path.join(OUTDIR_PROCESSED, DATASET, "{sample}_sorted.bam")
#     threads:
#         96
#     conda:
#         "envs/cami2_processing.yml"
#     shell:
#         """
#         strobealign -t {threads} {input.catalogue} {input.read} | samtools sort -o {output}
#         """

# rule vamb:
#     input:
#         catalogue=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz")),
#         bams=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "{sample}_sorted.bam"), sample=SAMPLES.get(DATASET, []))
#     output:
#         os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "vamb_output")
#     conda:
#         "envs/cami2_environment.yml"
#     shell:
#         """
#         vamb bin default --outdir {output} --fasta {input.catalogue} --bamfiles {input.bams}
#         """

rule semibin:
    input:
        catalogue=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz")),
        bams=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "{sample}_sorted.bam"), sample=SAMPLES.get(DATASET, []))
    output:
        os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "semibin_output")
    conda:
        "envs/semibin_environment.yml"
    shell:
        """
        
        """

rule comebin:
    input:
        catalogue=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz")),
        bams=expand(os.path.join(OUTDIR_PROCESSED, DATASET, "{sample}_sorted.bam"), sample=SAMPLES.get(DATASET, []))
    output:
        os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "comebin_output")
    conda:
        "envs/comebin_environment.yml"
    shell:
        """
        
        """




rule move_cluster_outputs:
    input:
        model_outputs=expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "{model}_output"), model=MODELS)
    output:
        cluster_results=directory(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "all_clusters"))
    conda:
        "envs/cami2_processing.yml"
    shell:
        """
        python {SRC_DIR}/move_cluster_outputs.py {output.cluster_results} {input.model_outputs}
        """


rule create_fasta_bins:
    input:
        catalogue=os.path.join(OUTDIR_PROCESSED, DATASET, "catalogue.fna.gz"),
        cluster_results=expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "all_clusters", "{model}_output_clusters.tsv"), model=MODELS)
    output:
        dirs=directory(expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "fasta_bins", "{model}"), model=MODELS))
    params:
        minsize_bins = 250000
    conda: 
        "envs/vamb.yml"
    shell:
        """
        python {SRC_DIR}/create_fasta.py {input.catalogue} {input.cluster_results} {params.minsize_bins} {output.dirs}
        """




rule download_checkm2_database:
    output:
        os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "checkm2_database_downloaded.txt")
    conda:
        "envs/checkm2.yml"
    params:
        database_dir="/home/ucloud/databases/CheckM2_database/uniref100.KO.1.dmnd"
    shell:
        """
        checkm2 database --download --setdblocation {params.database_dir}
        """


rule checkm2:
    input:
       fasta_bins=expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "fasta_bins", "{model}"), model=MODELS),
       checkm2_database_flag=os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "checkm2_database_downloaded.txt")
    output:
        output=directory(expand(os.path.join(OUTDIR_MODEL_RESULTS, DATASET, "checkm2", "{model}_results"), model=MODELS))
    conda:
        "envs/checkm2.yml"
    threads:
        96
    shell:
        """
        checkm2 predict --threads {threads} --input {input.fasta_bins} --output-directory {output.output}
        """













# rule concatenate_process_semibin:
#     input:
#         os.path.join(OUTDIR, DATASET, "catalogue.fna.gz")
#     output:
#         os.path.join(OUTDIR, DATASET, "semibin/catalogue_semibin.fna")
#     conda:
#         "envs/cami2_environment.yml"
#     shell:
#         """
#         zcat {input} | sed -E 's/([0-9]+)([A-Za-z])/\\1:\\2/' > {output} 
#         """


# rule bam_process_semibin:
#     input:
#         bam=os.path.join(OUTDIR, DATASET, "{sample}_sorted.bam")
#     output:
#         modified_bam=os.path.join(OUTDIR, DATASET, "semibin/{sample}_sorted_semibin.bam")
#     conda:
#         "envs/cami2_environment.yml"
#     shell:
#         """
#         samtools view -H {input.bam} | \\
#         sed -E 's/([0-9]+)([A-Za-z0-9]+)/\\1:\\2/' | \\
#         samtools reheader - {input.bam} > {output.modified_bam}
#         """

#rule_all
    #input:
        #os.path.join(OUTDIR, DATASET, "semibin/catalogue_semibin.fna")
        #expand(os.path.join(OUTDIR, DATASET, "semibin/{sample}_sorted_semibin.bam"), sample=[sample for sample in SAMPLES.get(DATASET, [])])





    

